{
 "metadata": {
  "name": "",
  "signature": "sha256:1f399e9b6150b6673f0b69b4f3ca9105b6215bdcd030ac8afcc0dfd6671436a7"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import findspark\n",
      "print findspark.find()\n",
      "findspark.init()\n",
      "\n",
      "import pyspark\n",
      "sc = pyspark.SparkContext(appName=\"Spark1\")\n",
      "\n",
      "import numpy as np \n",
      "import itertools"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/usr/local/opt/apache-spark/libexec\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def copartitioned(RDD1, RDD2):\n",
      "    \"check if two RDDs are copartitioned\"\n",
      "    return RDD1.partitioner == RDD2.partitioner"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#######\n",
      "# Implements Breadth First Search\n",
      "# Arguments:\n",
      "# \tadj (KV RDD): Edge list. For example: [(1,2), (1, 3), (2,3), (3, 2)] is the graph where there is a directed edge from 1 to 2 and 1 to 3, edges in both directions between 2 and 3.\n",
      "# \tstart (string): Where the breadth first search will start\n",
      "#   If a stopNode is provided, then it will stop iterating once that node is found.\n",
      "# Returns:\n",
      "# \t(RDD) Distances to each point, and (RDD) list of all points that were not reachable.\n",
      "def bfs(adj, start, sc, numPartitions, list_of_nodes, stopNode=None):\n",
      "    accum = sc.accumulator(0)\n",
      "    print 'Adjacency partitions:', adj.getNumPartitions()\n",
      "    adj = adj.map(lambda x: x).partitionBy(numPartitions).cache()\n",
      "    def reducer(tup):\n",
      "        left_val = tup[0]\n",
      "        right_vals = tup[1]\n",
      "        if len(right_vals) == 0:\n",
      "            return list(left_val)[0]\n",
      "        elif list(left_val)[0] == -1:\n",
      "            return list(right_vals)[0]\n",
      "        else:\n",
      "            return list(left_val)[0]\n",
      "    \n",
      "    def divider(tup):\n",
      "        dist = tup[0]\n",
      "        node_vals = tup[1]\n",
      "        return [(nv, dist + 1) for nv in node_vals]\n",
      "\n",
      "\n",
      "    distances = adj.mapValues(lambda _: -1)\n",
      "\n",
      "    traversed = sc.parallelize([(start, 0)]).cache()\n",
      "    distances = distances.fullOuterJoin(traversed).mapValues(lambda (x, y): x if y == None else y).partitionBy(numPartitions).cache()\n",
      "\n",
      "    adj = adj.partitionBy(numPartitions)\n",
      "    distances = distances.partitionBy(numPartitions, partitionFunc=adj.partitioner)\n",
      "    print distances.take(10)\n",
      "    print adj.take(10)\n",
      "    print distances.partitioner\n",
      "    print adj.partitioner\n",
      "    assert(copartitioned(adj, distances))\n",
      "    print 'Start traverse partitions:', distances.getNumPartitions()\n",
      "    farthest = 0\n",
      "    accum.add(1)\n",
      "    while accum.value != 0:\n",
      "        accum.value = 0\n",
      "        print \"\\n\\nOn iteration \", farthest, ' for ', start, '\\n\\n'\n",
      "        if stopNode != None and distances.lookup(stopNode)[0] != -1:\n",
      "            break\n",
      "        farthest_nodes = distances.filter(lambda (node, dist): dist == farthest)\n",
      "\n",
      "        joined_farthest_neighbors = farthest_nodes.join(adj, numPartitions)\n",
      "        neighbor_distances = joined_farthest_neighbors.values().flatMap(divider).partitionBy(numPartitions).cache()\n",
      "        assert(copartitioned(neighbor_distances, distances))\n",
      "        distances = distances.cogroup(neighbor_distances).mapValues(reducer).cache()\n",
      "        farthest += 1\n",
      "        distances.filter(lambda (x, dist): dist == farthest).foreach(lambda x: accum.add(1))\n",
      "        #print traversed.take(100)\n",
      "\n",
      "    print 'End tranverse partitions:', distances.getNumPartitions()\n",
      "    return distances, distances.filter(lambda (node, dist): dist < 0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Your code here\n",
      "#import pyspark\n",
      "#sc = pyspark.SparkContext(appName=\"Spark1\")\n",
      "#from P4_bfs import *\n",
      "\n",
      "# make spark shut the hell up\n",
      "logger = sc._jvm.org.apache.log4j\n",
      "logger.LogManager.getLogger(\"org\"). setLevel( logger.Level.ERROR )\n",
      "logger.LogManager.getLogger(\"akka\").setLevel( logger.Level.ERROR )\n",
      "\n",
      "import numpy as np \n",
      "import itertools\n",
      "\n",
      "def remove_quotes(s):\n",
      "    return s.replace('\"','')\n",
      "\n",
      "def get_pairs(arr):\n",
      "    return list(itertools.permutations(arr, 2))\n",
      "\n",
      "num_Partitions = 16\n",
      "\n",
      "wlist = sc.textFile('source.csv', num_Partitions)\n",
      "#Extract the strings and remove all unnessary quotes\n",
      "better = wlist.map(lambda x: x.split('\",\"'), True).map(lambda x: map(remove_quotes, x), True)\n",
      "character_to_book = better.map(lambda x: (x[0], x[1]))\n",
      "book_to_character = character_to_book.map(lambda (x, y): (y, x))\n",
      "book_to_characters = book_to_character.groupByKey()\n",
      "\n",
      "adjacent_heros = book_to_characters.flatMap(lambda (book, characters): get_pairs(characters)).distinct().partitionBy(num_Partitions)\n",
      "\n",
      "characters = adjacent_heros.groupByKey().mapValues(list).partitionBy(num_Partitions)\n",
      "\n",
      "res1, res2 = bfs(characters, 'CAPTAIN AMERICA', sc, None, None)\n",
      "print res1.values().countByValue(), '\\n'\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Adjacency partitions: 16\n",
        "[(u'PENTIGAAR', -1), (u'BOUNTY II', -1), (u'CRANDAL, BEN', -1), (u'ERWIN, MORLEY', -1), (u'MANACLE/', -1), (u'GENESIS/TYLER DAYSPR', -1), (u'DEFENSOR', -1), (u'DARK MOTHER/FINALITY', -1), (u'GREEN GOBLIN V/', -1), (u'DARK ONE', -1)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[(u'QUESADA, JOE', [u'SPIDER-MAN/PETER PAR', u'DAREDEVIL/MATT MURDO', u'DR. STRANGE/STEPHEN ', u'INVISIBLE WOMAN/SUE ', u'RALF', u'MILLER, FRANK', u'QUINN, ASHLEY', u'SMITH, KEVIN', u'OKOYE', u'MALICE V/NAKIA', u'MYSTERIO/QUENTIN BEC', u'ZURI', u'MR. FANTASTIC/REED R', u'LEE, STAN', u'BLAKE, BECKY', u'ROSS, EVERETT KENNET', u'GLADIATOR/MELVIN POT', u'BLACK WIDOW/NATASHA ', u'BUTCH', u'SHARPE, ROSALINDE', u'MCKENZIE, LYDIA', u'DAKESIAN, NANCI', u'THING/BENJAMIN J. GR', u'POTTER, BETSY BEATTY', u'EIGHTBALL', u'LAMY, KELLY', u'PAGE, KAREN', u'NELSON, FRANKLIN FOG', u'NELSON, CANDACE', u'MARTINEZ, ALITHA', u'DARLA', u\"BLACK PANTHER/T'CHAL\", u'PALMIOTTI, JIMMY', u'OSBORN, LIZ ALLAN', u'URICH, BEN', u'EVERETT, BILL', u'WATSON-PARKER, MARY ', u'HUMAN TORCH/JOHNNY S', u'CAPTAIN AMERICA']), (u'PENTIGAAR', [u'RYAN, THOMAS', u'KINCAID, DR. JANE FO', u'NEFFETHESK', u'VOLSTAGG', u'ODIN [ASGARDIAN]', u'VOLLA', u'OLSON, MRS.', u'FAIRMONT, HANNAH', u'HEIMDALL [ASGARDIAN]', u'TOOTHGNASHER', u'STARFOX/EROS', u'FANDRAL [ASGARDIAN]', u'SIF', u'MOONDRAGON/HEATHER D', u'CAPTAIN MARVEL II/MO', u'BALDER [ASGARDIAN]', u'KURSE/ALGRIM [ASGARD', u'BETA RAY BILL', u'TIGRA/GREER NELSON', u'HELA [ASGARDIAN]', u'HOGUN [ASGARDIAN]', u'MALEKITH/MALCOLM KEI', u'TOOTHGRINDER', u'QUASAR III/WENDELL V', u'THOR/DR. DONALD BLAK']), (u'REEVES, MAJOR', [u'THREETREES', u'TALBOT, GLENN', u'INVISIBLE WOMAN/SUE ', u'DAVIS, ABBY', u'HULK/DR. ROBERT BRUC', u'BENNETT, MITCH', u'BANNER, BETTY ROSS T', u'MR. FANTASTIC/REED R', u'PUMA/THOMAS FIREHEAR', u'THING/BENJAMIN J. GR', u'BENNETT, DANNY', u'ROSS, GEN. THADDEUS ', u'HUMAN TORCH/JOHNNY S']), (u'DE LA COURTE', [u'JUBILEE/JUBILATION L', u'LEECH', u'CHAMBER/JONOTHON STA', u'MONDO', u'PENANCE III', u'BLACK TOM CASSIDY', u'MONDO II', u'JUGGERNAUT/CAIN MARK', u'BANSHEE/SEAN CASSIDY', u'PENANCE/MONET ST. CR', u'GANCE, TABITHA TABBY', u'SHAW, SHINOBI', u'SKIN/ANGELO ESPINOSA', u'CORSI, TOM', u'FROST, CORDELIA', u'DELACORTE, ARTHUR', u'WHITE QUEEN/EMMA FRO', u'MADDICKS, ARTHUR ART']), (u'KURAGARI', [u'TAR', u'ANGEL/WARREN KENNETH', u'GOMURR', u'PSYLOCKE/ELISABETH B']), (u\"K'RIN\", [u'CRYSTAL [INHUMAN]', u'LILANDRA NERAMANI [S', u'HERCULES [GREEK GOD]', u\"DEATHCRY [SHI'AR]\", u'GYRICH, HENRY PETER', u'MARILLA [INHUMAN]', u'QUICKSILVER/PIETRO M', u'BLACK WIDOW/NATASHA ', u'JARVIS, EDWIN ']), (u'LANGFORD, DR. DAVID', [u'SPIDER-MAN/PETER PAR', u'FANG III', u'PILGRIM', u'BANCROFT, MARTINE', u'LILITH II', u'GHOST RIDER III/DAN ', u'MORBIUS/DR. MICHAEL ', u'WEISENTHAL, DR. JACO', u'BLACKOUT II/', u'NAKOTA', u'GHOST RIDER II/JOHNN']), (u'VISALIA', [u'MOONDRAGON/HEATHER D', u'JONES, RICHARD MILHO', u'MARIONETTE/MARI', u'JONES, MARLO CHANDLE', u'DEXAM', u'DRAX/ARTHUR DOUGLAS', u'BUG', u'RANN, COMMANDER ARCT', u'CAPTAIN MARVEL III/G']), (u'MASTER MENACE/DR. EM', [u'JONES, ANDY', u'JONES, DRUCILLA', u'NIGHTHAWK/KYLE RICHM', u'MOONGLOW/', u'INERTIA/', u'THERMITE/', u'JONES, PHILLIP', u'STEWART, TINA', u'GOLDEN ARCHER II/WYA', u'REDSTONE/', u'APE-X/XINA', u'HYPERION II', u'ARCANNA/ARCANNA JONE', u'HAYWIRE/', u'QUAGMIRE', u'STEWART, MADDIE', u'FOXFIRE/OLIVIA UNDER', u'LADY LARK/LINDA LEWI', u\"SKYMAX/SK'YM'X/JAMES\", u'DR. DECIBEL/ANTON DE', u'AMPHIBIAN/KINGLEY RI', u'NIGHTHAWK III/NEAL', u'WHIZZER II/STANLEY S', u'SHAPE', u'CROWLEY', u'MYSTERIUM/DR. JOSEPH', u'JONES, KATIE', u'HYPERION', u'TOM THUMB/THOMAS THO', u'THOR/DR. DONALD BLAK', u'CAPTAIN HAWK', u'DR. SPECTRUM/JOSEPH ', u'POWER PRINCESS/ZARDA']), (u'AMERICAN EAGLE II/JA', [u'SPIDER-MAN/PETER PAR', u'NIGHTHAWK/KYLE RICHM', u'BLACK KNIGHT V/DANE ', u'GOLDEN ARCHER II/WYA', u'QUICKSILVER/PIETRO M', u'VISION ', u'LADY LARK/LINDA LEWI', u'WHIZZER II/STANLEY S', u'SCARLET WITCH/WANDA ', u'HAWK', u'ARAGORN', u\"BLACK PANTHER/T'CHAL\", u'HYPERION', u'THOR/DR. DONALD BLAK', u'TOM THUMB/THOMAS THO', u'CAPTAIN AMERICA', u'CAPTAIN HAWK', u'DR. SPECTRUM/JOSEPH '])]\n",
        "<pyspark.rdd.Partitioner object at 0x113240b10>\n",
        "<pyspark.rdd.Partitioner object at 0x11055d7d0>\n"
       ]
      },
      {
       "ename": "AssertionError",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-16-63bcc1e3ec04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mcharacters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madjacent_heros\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_Partitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mres1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbfs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcharacters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CAPTAIN AMERICA'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mres1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcountByValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-15-f58b0995fdd1>\u001b[0m in \u001b[0;36mbfs\u001b[0;34m(adj, start, sc, numPartitions, list_of_nodes, stopNode)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopartitioned\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'Start traverse partitions:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mfarthest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mAssertionError\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#linklist = sc.textFile('../P5/links-simple-sorted.txt')\n",
      "#titlelist = sc.textFile('../P5/titles_sorted.txt')\n",
      "#linklist = sc.textFile('../P5/generated_links_sorted.txt')\n",
      "#titlelist = sc.textFile('../P5/generated_titles_sorted.txt')\n",
      "linklist = sc.textFile('../P5/generated_links_small_sorted.txt', 32)\n",
      "titlelist = sc.textFile('../P5/generated_titles_small_sorted.txt', 32)\n",
      "#linklist = sc.textFile('../P5/generated_links_representative_sorted.txt', 32)\n",
      "#titlelist = sc.textFile('../P5/generated_titles_representative_sorted.txt', 32)\n",
      "numerical_titles = titlelist.zipWithIndex().cache()\n",
      "\n",
      "num_nodes = numerical_titles.count()\n",
      "#num_Partitions = int(num_nodes/50)\n",
      "num_Partitions = 128\n",
      "\n",
      "# Borrowed from Professor's Github example on SparkPageRank\n",
      "def link_string_to_KV(s):\n",
      "    src, dests = s.split(': ')\n",
      "    dests = [int(to) - 1 for to in dests.split(' ')]\n",
      "    return (int(src) - 1, dests)\n",
      "\n",
      "split_list = linklist.map(link_string_to_KV).cache()\n",
      "nodes = split_list.map(lambda (x,y): int(x), True)\n",
      "assert(copartitioned(split_list, nodes))\n",
      "\n",
      "#start = \"Kevin_Bacon\"\n",
      "#end = \"Harvard_University\"\n",
      "start = 'TITLE_8'\n",
      "end = \"TITLE_4\"\n",
      "\n",
      "start_node = numerical_titles.lookup(start)[0]\n",
      "end_node = numerical_titles.lookup(end)[0]\n",
      "print start_node, end_node\n",
      "\n",
      "dist, unreachables = bfs(split_list, start_node, sc, num_Partitions, None, None)\n",
      "#print distances.values().countByValue(), '\\n'\n",
      "print distances.take(100)\n",
      "print \"Distance to end node:\", dist.map(lambda x: x).lookup(end_node)[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "7 3\n",
        "Adjacency partitions: 42\n",
        "[(0, -1), (1, -1), (2, -1), (3, -1), (4, -1), (5, -1), (6, -1), (7, 0), (8, -1)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[(0, [5, 4]), (1, [5]), (2, [1, 0, 3, 8, 2, 7, 6, 4, 5]), (3, [8, 1, 7, 2, 5]), (4, [4, 7, 2, 5, 3, 0, 8, 6]), (5, [8, 3, 1, 4]), (6, [3, 5]), (7, [7, 5, 0, 8, 1, 6, 2, 4]), (8, [2, 5, 4, 0, 3, 6])]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "<pyspark.rdd.Partitioner object at 0x11387e210>\n",
        "<pyspark.rdd.Partitioner object at 0x113897090>\n",
        "Start traverse partitions: 128\n",
        "\n",
        "\n",
        "On iteration  0  for  7 \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "On iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1  for  7 \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "On iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2  for  7 \n",
        "\n",
        "\n",
        "End tranverse partitions:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 128\n",
        "[(0, 1), (1, 1), (2, 1), (3, 2), (4, 1), (5, 1), (6, 1), (7, 0), (8, 1)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Distance to end node: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x_test = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      "y_test = sc.parallelize([(\"a\", 2), (\"a\", 2), (\"c\", 8)])\n",
      "\n",
      "def reducer(tup):\n",
      "    left_val = tup[0]\n",
      "    right_vals = tup[1]\n",
      "    if len(left_val) == 0:\n",
      "        return list(right_vals)[0]\n",
      "    elif len(right_vals) == 0:\n",
      "        return list(left_val)[0]\n",
      "    else:\n",
      "        return list(left_val)[0]\n",
      "\n",
      "\n",
      "wut1 = sc.parallelize([(1, 2), (1, 1), (1, 0), (1, 8), (1, 4), (7, 4), \n",
      "                       (8, 0), (8, 6), (8, 8), (8, 1), (8, 4), (8, 2), \n",
      "                       (3, 0), (3, 2), (3, 6), (3, 5), (3, 1), (3, 7), \n",
      "                       (3, 8), (3, 3), (0, 2), (0, 3), (0, 4), (0, 5), \n",
      "                       (0, 6), (0, 1), (0, 0), (0, 8), (4, 0), (4, 8), \n",
      "                       (4, 1), (4, 2), (4, 5), (4, 6), (4, 3), (6, 8), \n",
      "                       (6, 6), (6, 0), (6, 3), (6, 4), (6, 1), (6, 7), \n",
      "                       (6, 5), (6, 2), (5, 7), (5, 1), (5, 6), (5, 3), \n",
      "                       (5, 4), (5, 2), (5, 5), (5, 8), (2, 1), (2, 0), (2, 6)])\n",
      "wut2 = sc.parallelize([(7, 0)])\n",
      "\n",
      "print wut1.join(wut2).collect()\n",
      "#print x_test.cogroup(y_test).mapValues(reducer).collect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(7, (4, 0))]\n"
       ]
      }
     ],
     "prompt_number": 123
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}