{
 "metadata": {
  "name": "",
  "signature": "sha256:feb17547f55483655d9ae19564df9dd1492214521d4a25f6fcaf6dd4f9bf0159"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import findspark\n",
      "print findspark.find()\n",
      "findspark.init()\n",
      "\n",
      "import pyspark\n",
      "sc = pyspark.SparkContext(appName=\"Spark1\")\n",
      "\n",
      "import numpy as np \n",
      "import itertools"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/usr/local/opt/apache-spark/libexec\n"
       ]
      },
      {
       "ename": "ValueError",
       "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=Spark1, master=local[*]) created by __init__ at <ipython-input-1-f372b3536f92>:6 ",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-14-f372b3536f92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Spark1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \"\"\"\n\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
        "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway)\u001b[0m\n\u001b[1;32m    248\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 250\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    251\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Spark1, master=local[*]) created by __init__ at <ipython-input-1-f372b3536f92>:6 "
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def copartitioned(RDD1, RDD2):\n",
      "    \"check if two RDDs are copartitioned\"\n",
      "    return RDD1.partitioner == RDD2.partitioner"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#######\n",
      "# Implements Breadth First Search\n",
      "# Arguments:\n",
      "# \tadj (KV RDD): Edge list. For example: [(1,2), (1, 3), (2,3), (3, 2)] is the graph where there is a directed edge from 1 to 2 and 1 to 3, edges in both directions between 2 and 3.\n",
      "# \tstart (string): Where the breadth first search will start\n",
      "#   If a stopNode is provided, then it will stop iterating once that node is found.\n",
      "# Returns:\n",
      "# \t(RDD) Distances to each point, and (RDD) list of all points that were not reachable.\n",
      "def bfs(adj, start, sc, numPartitions, distances=None, stopNode=None):\n",
      "    accum = sc.accumulator(0)\n",
      "    print 'Adjacency partitions:', adj.getNumPartitions()\n",
      "    adj = adj.map(lambda x: x).partitionBy(numPartitions).cache()\n",
      "    def reducer(tup):\n",
      "        left_val = tup[0]\n",
      "        right_vals = tup[1]\n",
      "        if len(right_vals) == 0:\n",
      "            return list(left_val)[0]\n",
      "        elif list(left_val)[0] == -1:\n",
      "            return list(right_vals)[0]\n",
      "        else:\n",
      "            return list(left_val)[0]\n",
      "    \n",
      "    def divider(tup):\n",
      "        dist = tup[0]\n",
      "        node_vals = tup[1]\n",
      "        return [(nv, dist + 1) for nv in node_vals]\n",
      "\n",
      "    if distances == None:\n",
      "        distances = adj.mapValues(lambda _: -1)\n",
      "\n",
      "    traversed = sc.parallelize([(start, 0)]).cache().partitionBy(numPartitions)\n",
      "    distances = distances.fullOuterJoin(traversed).mapValues(lambda (x, y): x if y == None else y).partitionBy(numPartitions).cache()\n",
      "    #print distances.take(100)\n",
      "    assert(copartitioned(adj, distances))\n",
      "    print 'Start traverse partitions:', distances.getNumPartitions()\n",
      "    farthest = 0\n",
      "    accum.add(1)\n",
      "    while accum.value != 0:\n",
      "        accum.value = 0\n",
      "        print \"\\n\\nOn iteration \", farthest, ' for ', start, '\\n\\n'\n",
      "        if stopNode != None and distances.lookup(stopNode)[0] != -1:\n",
      "            break\n",
      "        farthest_nodes = distances.filter(lambda (node, dist): dist == farthest)\n",
      "\n",
      "        joined_farthest_neighbors = farthest_nodes.join(adj, numPartitions)\n",
      "        neighbor_distances = joined_farthest_neighbors.values().flatMap(divider).partitionBy(numPartitions).cache()\n",
      "        assert(copartitioned(neighbor_distances, distances))\n",
      "        distances = distances.cogroup(neighbor_distances).mapValues(reducer).cache()\n",
      "        farthest += 1\n",
      "        distances.filter(lambda (x, dist): dist == farthest).foreach(lambda x: accum.add(1))\n",
      "        #print traversed.take(100)\n",
      "\n",
      "    print 'End tranverse partitions:', distances.getNumPartitions()\n",
      "    return distances, distances.filter(lambda (node, dist): dist < 0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## This is for the connected components\n",
      "# Approach: BFS until it quits. Then reduce the distances to only the ones that are unreachable. Increment counter by 1. Restart BFS with the new set of distances.\n",
      "\n",
      "def count_connected_components(adj, numPartitions, sc):\n",
      "    adj = adj.map(lambda x: x).partitionBy(numPartitions).cache()\n",
      "    dists = adj.mapValues(lambda _: -1)\n",
      "    num_left = sc.accumulator(0)\n",
      "    dists.foreach(lambda x: num_left.add(1))\n",
      "    num_conn_components = 0\n",
      "    while(num_left.value > 0):\n",
      "        num_conn_components += 1\n",
      "        num_left.value = 0\n",
      "        start = dists.takeSample(True, 1)[0][0]\n",
      "        dist, unreachable = bfs(adj, start, sc, numPartitions, distances=dists, stopNode=None)\n",
      "        dists = dist.filter(lambda (node, dist): dist < 0)\n",
      "        dists.foreach(lambda x: num_left.add(1))\n",
      "        \n",
      "    return num_conn_components"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Your code here\n",
      "#import pyspark\n",
      "#sc = pyspark.SparkContext(appName=\"Spark1\")\n",
      "#from P4_bfs import *\n",
      "\n",
      "# make spark shut the hell up\n",
      "logger = sc._jvm.org.apache.log4j\n",
      "logger.LogManager.getLogger(\"org\"). setLevel( logger.Level.ERROR )\n",
      "logger.LogManager.getLogger(\"akka\").setLevel( logger.Level.ERROR )\n",
      "\n",
      "import numpy as np \n",
      "import itertools\n",
      "\n",
      "def remove_quotes(s):\n",
      "    return s.replace('\"','')\n",
      "\n",
      "def get_pairs(arr):\n",
      "    return list(itertools.permutations(arr, 2))\n",
      "\n",
      "num_Partitions = 15\n",
      "\n",
      "wlist = sc.textFile('source.csv', num_Partitions)\n",
      "#Extract the strings and remove all unnessary quotes\n",
      "better = wlist.map(lambda x: x.split('\",\"'), True).map(lambda x: map(remove_quotes, x), True)\n",
      "character_to_book = better.map(lambda x: (x[0], x[1]))\n",
      "book_to_character = character_to_book.map(lambda (x, y): (y, x))\n",
      "book_to_characters = book_to_character.groupByKey()\n",
      "\n",
      "adjacent_heros = book_to_characters.flatMap(lambda (book, characters): get_pairs(characters)).distinct().partitionBy(num_Partitions)\n",
      "\n",
      "characters = adjacent_heros.groupByKey().partitionBy(num_Partitions)\n",
      "print characters.take(20)\n",
      "\n",
      "res1, res2 = bfs(characters, 'CAPTAIN AMERICA', sc, num_Partitions, distances=None, stopNode=None)\n",
      "print res1.values().countByValue(), '\\n'\n",
      "\n",
      "print '\\n\\nCalculating connected components....\\n\\n'\n",
      "num_conn = count_connected_components(characters, num_Partitions, sc)\n",
      "print '\\n\\n\\n\\n\\n\\nNumber connected components: ', num_conn"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(u'SIR', <pyspark.resultiterable.ResultIterable object at 0x107b27750>), (u'HYZAKTL', <pyspark.resultiterable.ResultIterable object at 0x107b27f50>), (u'GAMBIT/REMY LEBEAU ', <pyspark.resultiterable.ResultIterable object at 0x107b27290>), (u'POITIER, CLAUDE', <pyspark.resultiterable.ResultIterable object at 0x107b27e50>), (u'NINGAL', <pyspark.resultiterable.ResultIterable object at 0x107b27310>), (u'HIGGINS, RICHIE', <pyspark.resultiterable.ResultIterable object at 0x107b27250>), (u'GARTHAN SAAL', <pyspark.resultiterable.ResultIterable object at 0x107b27590>), (u'FLATMAN', <pyspark.resultiterable.ResultIterable object at 0x107b27150>), (u'PROFESSOR X | MUTANT', <pyspark.resultiterable.ResultIterable object at 0x107b27e10>), (u'BENWAY, DR.', <pyspark.resultiterable.ResultIterable object at 0x107b27c10>), (u'KANDRA', <pyspark.resultiterable.ResultIterable object at 0x107b27690>), (u'HEADLOK', <pyspark.resultiterable.ResultIterable object at 0x107a1ddd0>), (u'JONES, SANDY', <pyspark.resultiterable.ResultIterable object at 0x107a1d190>), (u'RITTER, DEBORAH ', <pyspark.resultiterable.ResultIterable object at 0x107aa7d10>), (u'MANDARIN', <pyspark.resultiterable.ResultIterable object at 0x107aa7d50>), (u'BULLET BIKER', <pyspark.resultiterable.ResultIterable object at 0x107aa7990>), (u'DR. FAUSTUS', <pyspark.resultiterable.ResultIterable object at 0x107aa7910>), (u'GREEN GOBLIN V/', <pyspark.resultiterable.ResultIterable object at 0x107aa7750>), (u'ENRAGED', <pyspark.resultiterable.ResultIterable object at 0x107aa7110>), (u'RED WOLF III/WILL TA', <pyspark.resultiterable.ResultIterable object at 0x107aa7e10>)]\n",
        "Adjacency partitions: 15\n",
        "Start traverse partitions:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 15\n",
        "\n",
        "\n",
        "On iteration  0  for  CAPTAIN AMERICA \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "On iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1  for  CAPTAIN AMERICA \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "On iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2  for  CAPTAIN AMERICA \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "On iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3  for  CAPTAIN AMERICA \n",
        "\n",
        "\n",
        "End tranverse partitions:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 15\n",
        "defaultdict(<type 'int'>, {0: 1, 1: 1906, 2: 4463, 3: 38, -1: 18})"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \n",
        "\n",
        "\n",
        "\n",
        "Calculating connected components....\n",
        "\n",
        "\n",
        "Adjacency partitions:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 15\n",
        "Start traverse partitions:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 15\n",
        "\n",
        "\n",
        "On iteration  0  for  KAFKA, DR. ASHLEY \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "On iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1  for  KAFKA, DR. ASHLEY \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "On iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2  for  KAFKA, DR. ASHLEY \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "On iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3  for  KAFKA, DR. ASHLEY \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "On iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4  for  KAFKA, DR. ASHLEY \n",
        "\n",
        "\n",
        "End tranverse partitions:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 15\n",
        "Adjacency partitions:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 15\n",
        "Start traverse partitions:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 15\n",
        "\n",
        "\n",
        "On iteration  0  for  ASHER, MICHAEL \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "On iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1  for  ASHER, MICHAEL \n",
        "\n",
        "\n",
        "End tranverse partitions:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 15\n",
        "Adjacency partitions:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 15\n",
        "Start traverse partitions:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 15\n",
        "\n",
        "\n",
        "On iteration  0  for  PANTHER CUB/ \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "On iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1  for  PANTHER CUB/ \n",
        "\n",
        "\n",
        "End tranverse partitions:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 15\n",
        "Adjacency partitions:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 15\n",
        "Start traverse partitions:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 15\n",
        "\n",
        "\n",
        "On iteration  0  for  STEEL SPIDER/OLLIE O \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "On iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1  for  STEEL SPIDER/OLLIE O \n",
        "\n",
        "\n",
        "End tranverse partitions:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 15\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Number connected components: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#linklist = sc.textFile('../P5/generated_links_representative_sorted.txt', 32)\n",
      "#titlelist = sc.textFile('../P5/generated_titles_representative_sorted.txt', 32)\n",
      "linklist = sc.textFile('../P5/generated_links_small_sorted.txt', 32)\n",
      "titlelist = sc.textFile('../P5/generated_titles_small_sorted.txt', 32)\n",
      "#linklist = sc.textFile('../P5/links-simple-sorted.txt')\n",
      "#titlelist = sc.textFile('../P5/titles_sorted.txt')\n",
      "#linklist = sc.textFile('../P5/generated_links_sorted.txt')\n",
      "#titlelist = sc.textFile('../P5/generated_titles_sorted.txt')\n",
      "numerical_titles = titlelist.zipWithIndex().cache()\n",
      "\n",
      "num_nodes = numerical_titles.count()\n",
      "#num_Partitions = int(num_nodes/50)\n",
      "num_Partitions = 128\n",
      "\n",
      "# Borrowed from Professor's Github example on SparkPageRank\n",
      "def link_string_to_KV(s):\n",
      "    src, dests = s.split(': ')\n",
      "    dests = [int(to) - 1 for to in dests.split(' ')]\n",
      "    return (int(src) - 1, dests)\n",
      "\n",
      "split_list = linklist.map(link_string_to_KV).cache()\n",
      "nodes = split_list.map(lambda (x,y): int(x), True)\n",
      "assert(copartitioned(split_list, nodes))\n",
      "\n",
      "#start = \"Kevin_Bacon\"\n",
      "#end = \"Harvard_University\"\n",
      "start = 'TITLE_8'\n",
      "end = \"TITLE_4\"\n",
      "\n",
      "start_node = numerical_titles.lookup(start)[0]\n",
      "end_node = numerical_titles.lookup(end)[0]\n",
      "print start_node, end_node\n",
      "\n",
      "dist, unreachables = bfs(split_list, start_node, sc, num_Partitions, distances=None, stopNode=None)\n",
      "#print distances.values().countByValue(), '\\n'\n",
      "print dist.take(100)\n",
      "print \"Distance to end node:\", dist.map(lambda x: x).lookup(end_node)[0]\n",
      "\n",
      "print '\\n\\nCalculating connected components....\\n\\n'\n",
      "num_conn = count_connected_components(split_list, num_Partitions, sc)\n",
      "print '\\n\\n\\n\\n\\n\\nNumber connected components: ', num_conn"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "7 3\n",
        "Adjacency partitions: 42\n",
        "Start traverse partitions:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 128\n",
        "\n",
        "\n",
        "On iteration  0  for  7 \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "On iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1  for  7 \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "On iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2  for  7 \n",
        "\n",
        "\n",
        "End tranverse partitions:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 128\n",
        "[(0, 1), (1, 1), (2, 1), (3, 2), (4, 1), (5, 1), (6, 1), (7, 0), (8, 1)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Distance to end node: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2\n",
        "\n",
        "\n",
        "Calculating connected components....\n",
        "\n",
        "\n",
        "Adjacency partitions:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 128\n",
        "Start traverse partitions:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 128\n",
        "\n",
        "\n",
        "On iteration  0  for  4 \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "On iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1  for  4 \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "On iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2  for  4 \n",
        "\n",
        "\n",
        "End tranverse partitions:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 128\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Number connected components: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x_test = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      "y_test = sc.parallelize([(\"a\", 2), (\"a\", 2), (\"c\", 8)])\n",
      "\n",
      "def reducer(tup):\n",
      "    left_val = tup[0]\n",
      "    right_vals = tup[1]\n",
      "    if len(left_val) == 0:\n",
      "        return list(right_vals)[0]\n",
      "    elif len(right_vals) == 0:\n",
      "        return list(left_val)[0]\n",
      "    else:\n",
      "        return list(left_val)[0]\n",
      "\n",
      "\n",
      "wut1 = sc.parallelize([(1, 2), (1, 1), (1, 0), (1, 8), (1, 4), (7, 4), \n",
      "                       (8, 0), (8, 6), (8, 8), (8, 1), (8, 4), (8, 2), \n",
      "                       (3, 0), (3, 2), (3, 6), (3, 5), (3, 1), (3, 7), \n",
      "                       (3, 8), (3, 3), (0, 2), (0, 3), (0, 4), (0, 5), \n",
      "                       (0, 6), (0, 1), (0, 0), (0, 8), (4, 0), (4, 8), \n",
      "                       (4, 1), (4, 2), (4, 5), (4, 6), (4, 3), (6, 8), \n",
      "                       (6, 6), (6, 0), (6, 3), (6, 4), (6, 1), (6, 7), \n",
      "                       (6, 5), (6, 2), (5, 7), (5, 1), (5, 6), (5, 3), \n",
      "                       (5, 4), (5, 2), (5, 5), (5, 8), (2, 1), (2, 0), (2, 6)])\n",
      "wut2 = sc.parallelize([(7, 0)])\n",
      "\n",
      "print wut1.join(wut2).collect()\n",
      "#print x_test.cogroup(y_test).mapValues(reducer).collect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(7, (4, 0))]\n"
       ]
      }
     ],
     "prompt_number": 123
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}