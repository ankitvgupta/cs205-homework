{
 "metadata": {
  "name": "",
  "signature": "sha256:01a06fcd822c22cf27482d4a70f3280e5b1fb8a04857ccf3295f3afafccf8015"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import findspark\n",
      "print findspark.find()\n",
      "findspark.init()\n",
      "\n",
      "import pyspark\n",
      "sc = pyspark.SparkContext(appName=\"Spark1\")\n",
      "\n",
      "import numpy as np \n",
      "import itertools"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/usr/local/opt/apache-spark/libexec\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def copartitioned(RDD1, RDD2):\n",
      "    \"check if two RDDs are copartitioned\"\n",
      "    return RDD1.partitioner == RDD2.partitioner"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#######\n",
      "# Implements Breadth First Search\n",
      "# Arguments:\n",
      "# \tadj (KV RDD): Edge list. For example: [(1,2), (1, 3), (2,3), (3, 2)] is the graph where there is a directed edge from 1 to 2 and 1 to 3, edges in both directions between 2 and 3.\n",
      "# \tstart (string): Where the breadth first search will start\n",
      "# Returns:\n",
      "# \t(RDD) Distances to each point, and (RDD) list of all points that were not reachable.\n",
      "def bfs(adj, start, sc, numPartitions, list_of_nodes):\n",
      "    accum = sc.accumulator(0)\n",
      "    print 'Adjacency partitions:', adj.getNumPartitions()\n",
      "    adj = adj.map(lambda x: x).partitionBy(numPartitions).cache()\n",
      "    def reduceFun(tup):\n",
      "        #global accum\n",
      "        if tup[0] == None:\n",
      "            #accum.add(1)\n",
      "            return tup[1]\n",
      "        elif tup[1] == None:\n",
      "            return tup[0]\n",
      "        else:\n",
      "            return min(tup[0], tup[1])\n",
      "\n",
      "    def reduceFun2(tup):\n",
      "        if tup[1] == None:\n",
      "            return tup[0]\n",
      "        else:\n",
      "            return max(tup[0],tup[1])\n",
      "    def reduceFun3(tup):\n",
      "        # If a node isnt a neighbor, don't update it\n",
      "        if tup[1] == None:\n",
      "            return tup[0]\n",
      "        # If it was found as a neighbor, and was originally not found, keep it\n",
      "        elif tup[0] == None:\n",
      "            return tup[1]\n",
      "        else:\n",
      "            return min(tup[0], tup[1])\n",
      "    def reducer(tup):\n",
      "        left_val = tup[0]\n",
      "        right_vals = tup[1]\n",
      "        if len(left_val) == 0:\n",
      "            return list(right_vals)[0]\n",
      "        elif len(right_vals) == 0:\n",
      "            return list(left_val)[0]\n",
      "        else:\n",
      "            return list(left_val)[0]\n",
      "\n",
      "\n",
      "\n",
      "    #distances = adj.keys().distinct().map(lambda node: (node, -1.0), True).partitionBy(numPartitions)\n",
      "    distances = list_of_nodes.map(lambda node: (node, -1.0), True).partitionBy(numPartitions)\n",
      "    #print adj.partitioner\n",
      "    #print distances.partitioner\n",
      "    assert(copartitioned(adj, distances))\n",
      "    traversed = sc.parallelize([(start, 0)]).cache().partitionBy(numPartitions)\n",
      "    print 'Start traverse partitions:', traversed.getNumPartitions()\n",
      "    farthest = 0\n",
      "    accum.add(1)\n",
      "    while accum.value != 0:\n",
      "        accum.value = 0\n",
      "        print \"\\n\\nOn iteration \", farthest, ' for ', start, '\\n\\n'\n",
      "        # get all of the neighboring superheros\n",
      "        #\tStart by filtering for the ones that are farthest away\n",
      "        # \tThen join this with the adjacency matrix, which gives you all of the places it can go\n",
      "        #\tGet just the neighbors by using .values()\n",
      "        # \tGet rid of the distance value from the left side\n",
      "        #\tKeep only the unique ones using distinct (since there could be multipe ways to reach a node)\n",
      "        #\tAdd the distance to them, using a lambda that makes the neighbors into KVs with Key = name, Value = dist\n",
      "        #neighbors = traversed.filter(lambda (node, dist): dist == farthest).join(adj, numPartitions).values().values().distinct().map(lambda x: (x, farthest + 1), True)\n",
      "        #neighbors = traversed.filter(lambda (node, dist): dist == farthest).join(adj, numPartitions).values().values().map(lambda x: (x, farthest + 1), True).partitionBy(numPartitions)\n",
      "        #assert(copartitioned(neighbors, traversed))\n",
      "        #neighbors = traversed.filter(lambda (node, dist): dist == farthest).map(lambda x: x).join(adj, numPartitions).values().values().map(lambda x: (x, farthest + 1), True).partitionBy(numPartitions)\n",
      "        #assert(copartitioned(neighbors, traversed))\n",
      "        filtered = traversed.filter(lambda (node, dist): dist == farthest)\n",
      "        assert(copartitioned(filtered, adj))\n",
      "        #neighbors = filtered.join(adj, numPartitions).values().values().map(lambda x: (x, farthest + 1), True).partitionBy(numPartitions)\n",
      "        neighbors = filtered.join(adj, numPartitions).values().values().map(lambda x: (x, farthest + 1), True).partitionBy(numPartitions)\n",
      "        \n",
      "        assert(copartitioned(neighbors, traversed))\n",
      "\n",
      "\n",
      "        #neighbors = filtered.join(adj, numPartitions).map(lambda (x, (y, z)): (z, farthest + 1), True)\n",
      "        #print filtered.join(adj, numPartitions).map(lambda (x, (y, z)): (z, farthest + 1), True).take(100)\n",
      "\n",
      "        #print \"wut\"\n",
      "        #print adj.join(filtered).take(100)\n",
      "        #assert(copartitioned(neighbors, list_of_nodes))\n",
      "        #print \"wut\"\n",
      "        #print adj.take(100)\n",
      "        #print traversed.filter(lambda (node, dist): dist == farthest).take(100)\n",
      "        #fltrd = traversed.filter(lambda (node, dist): dist == farthest).map(lambda x: x)\n",
      "        #print fltrd.take(100)\n",
      "        #print fltrd.join(adj).take(100)\n",
      "        #assert(copartitioned(fltrd, adj))\n",
      "        \n",
      "        #traversed = traversed.fullOuterJoin(neighbors, numPartitions).mapValues(reduceFun3)\n",
      "        traversed = traversed.cogroup(neighbors).mapValues(reducer).cache()\n",
      "        #print traversed.take(10)\n",
      "        # combine the neighbors with what we already had, removing values we have already seen.\n",
      "        #traversed = traversed.fullOuterJoin(neighbors, numPartitions).mapValues(reduceFun)\n",
      "        #traversed.count()\n",
      "        # force the eval to calculate the accum values.\n",
      "        #print traversed.take(100)\n",
      "        farthest += 1\n",
      "        traversed.filter(lambda (x, dist): dist == farthest).foreach(lambda x: accum.add(1))\n",
      "        #print traversed.take(100)\n",
      "\n",
      "    print 'End tranverse partitions:', traversed.getNumPartitions()\n",
      "    traversed = traversed.map(lambda x: x)\n",
      "    final_vals = distances.leftOuterJoin(traversed).mapValues(reduceFun2)\n",
      "    #print 'finals'\n",
      "    #print final_vals.take(100)\n",
      "    #print \"Distance Distribution for \", start \n",
      "    #print final_vals.values().countByValue(), '\\n'\n",
      "    return final_vals, final_vals.filter(lambda (node, dist): dist < 0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Your code here\n",
      "#import pyspark\n",
      "#sc = pyspark.SparkContext(appName=\"Spark1\")\n",
      "#from P4_bfs import *\n",
      "\n",
      "# make spark shut the hell up\n",
      "logger = sc._jvm.org.apache.log4j\n",
      "logger.LogManager.getLogger(\"org\"). setLevel( logger.Level.ERROR )\n",
      "logger.LogManager.getLogger(\"akka\").setLevel( logger.Level.ERROR )\n",
      "\n",
      "import numpy as np \n",
      "import itertools\n",
      "\n",
      "def remove_quotes(s):\n",
      "    return s.replace('\"','')\n",
      "\n",
      "def get_pairs(arr):\n",
      "    return list(itertools.permutations(arr, 2))\n",
      "\n",
      "num_Partitions = 15\n",
      "\n",
      "wlist = sc.textFile('source.csv', num_Partitions)\n",
      "#Extract the strings and remove all unnessary quotes\n",
      "better = wlist.map(lambda x: x.split('\",\"'), True).map(lambda x: map(remove_quotes, x), True)\n",
      "character_to_book = better.map(lambda x: (x[0], x[1]))\n",
      "book_to_character = character_to_book.map(lambda (x, y): (y, x))\n",
      "book_to_characters = book_to_character.groupByKey()\n",
      "\n",
      "adjacent_heros = book_to_characters.flatMap(lambda (book, characters): get_pairs(characters)).distinct().partitionBy(num_Partitions)\n",
      "\n",
      "characters = adjacent_heros.groupByKey().partitionBy(num_Partitions).keys()\n",
      "print characters.take(20)\n",
      "\n",
      "res1, res2 = bfs(adjacent_heros, 'CAPTAIN AMERICA', sc, num_Partitions, characters)\n",
      "print res1.values().countByValue(), '\\n'\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'SIR', u'HYZAKTL', u'GAMBIT/REMY LEBEAU ', u'POITIER, CLAUDE', u'NINGAL', u'HIGGINS, RICHIE', u'GARTHAN SAAL', u'FLATMAN', u'PROFESSOR X | MUTANT', u'BENWAY, DR.', u'KANDRA', u'HEADLOK', u'JONES, SANDY', u'RITTER, DEBORAH ', u'MANDARIN', u'BULLET BIKER', u'DR. FAUSTUS', u'GREEN GOBLIN V/', u'ENRAGED', u'RED WOLF III/WILL TA']\n",
        "Adjacency partitions: 15\n",
        "Start traverse partitions: 15\n",
        "\n",
        "\n",
        "On iteration  0  for  CAPTAIN AMERICA \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "On iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1  for  CAPTAIN AMERICA \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "On iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2  for  CAPTAIN AMERICA \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "On iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3  for  CAPTAIN AMERICA \n",
        "\n",
        "\n",
        "End tranverse partitions:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 75\n",
        "defaultdict(<type 'int'>, {0: 1, 1: 1906, 2: 4463, 3: 38, -1.0: 18})"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \n",
        "\n"
       ]
      }
     ],
     "prompt_number": 203
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#linklist = sc.textFile('../P5/links-simple-sorted.txt')\n",
      "#titlelist = sc.textFile('../P5/titles_sorted.txt')\n",
      "#linklist = sc.textFile('../P5/generated_links_sorted.txt')\n",
      "#titlelist = sc.textFile('../P5/generated_titles_sorted.txt')\n",
      "#linklist = sc.textFile('../P5/generated_links_small_sorted.txt')\n",
      "#titlelist = sc.textFile('../P5/generated_titles_small_sorted.txt')\n",
      "linklist = sc.textFile('../P5/generated_links_representative_sorted.txt')\n",
      "titlelist = sc.textFile('../P5/generated_titles_representative_sorted.txt')\n",
      "numerical_titles = titlelist.zipWithIndex()\n",
      "#print numerical_titles.take(10)\n",
      "\n",
      "num_nodes = numerical_titles.count()\n",
      "#num_Partitions = int(num_nodes/50)\n",
      "num_Partitions = 100\n",
      "#num_Partitions = 2\n",
      "\n",
      "\n",
      "split_list = linklist.map(lambda x: x.split(':')).partitionBy(num_Partitions)\n",
      "#print split_list.take(10)\n",
      "nodes = split_list.partitionBy(num_Partitions).map(lambda (x,y): int(x) - 1, True)\n",
      "#print nodes.collect()\n",
      "#print nodes.take(10)\n",
      "\n",
      "#print split_list.take(10)\n",
      "\n",
      "\n",
      "# Create an RDD of directed edges, and subtract 1 from every edge to make it 0 indexed.\n",
      "adj = split_list.flatMapValues(lambda x: x.split()).map(lambda (n1, n2): (int(n1) -1 , int(n2) -1), True)\n",
      "#print adj.take(1000)\n",
      "#print nodes.take(100)\n",
      "assert(copartitioned(split_list, adj))\n",
      "#adj = split_list.mapValues(lambda x: x.split()).map(lambda (str_x, str_neighbors): (int(str_x) - 1, map(lambda x: int(x) - 1, str_neighbors)))\n",
      "#adj = split_list.flatMapValues(lambda x: x.split()).map(lambda (n1, n2): (int(n1) -1 , int(n2) -1)).groupByKey()\n",
      "\n",
      "#print adj.take(10)\n",
      "#print edges.take(100)\n",
      "#start = \"Kevin_Bacon\"\n",
      "#end = \"Harvard_University\"\n",
      "start = 'TITLE_8'\n",
      "end = \"TITLE_5\"\n",
      "\n",
      "start_node = numerical_titles.lookup(start)[0]\n",
      "end_node = numerical_titles.lookup(end)[0]\n",
      "print start_node, end_node\n",
      "\n",
      "distances, unreachables = bfs(adj, start_node, sc, num_Partitions, nodes)\n",
      "#print distances.values().countByValue(), '\\n'\n",
      "print distances.take(100)\n",
      "print \"Distance to end node:\", distances.map(lambda x: x).lookup(end_node)[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "7 4\n",
        "Adjacency partitions: 20\n",
        "Start traverse partitions: 20\n",
        "\n",
        "\n",
        "On iteration  0  for  7 \n",
        "\n",
        "\n"
       ]
      },
      {
       "ename": "Py4JJavaError",
       "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job 24 cancelled because Stage 43 was cancelled\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1280)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1226)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply$mcVI$sp(DAGScheduler.scala:1214)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply(DAGScheduler.scala:1213)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply(DAGScheduler.scala:1213)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofInt.foreach(ArrayOps.scala:156)\n\tat org.apache.spark.scheduler.DAGScheduler.handleStageCancellation(DAGScheduler.scala:1213)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1466)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1455)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1444)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1813)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1826)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1839)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1910)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:905)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:904)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:373)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:483)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\n",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-11-4d00d807295a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mstart_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mdistances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munreachables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbfs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_Partitions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;31m#print distances.values().countByValue(), '\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-10-27361b645a22>\u001b[0m in \u001b[0;36mbfs\u001b[0;34m(adj, start, sc, numPartitions, list_of_nodes)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m#print traversed.take(100)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mfarthest\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mtraversed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfarthest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforeach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0maccum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;31m#print traversed.take(100)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mforeach\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    747\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessPartition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Force evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforeachPartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1004\u001b[0m         \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \"\"\"\n\u001b[0;32m-> 1006\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36msum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    995\u001b[0m         \u001b[0;36m6.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m         \"\"\"\n\u001b[0;32m--> 997\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mfold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;31m# to the final reduce call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    771\u001b[0m         \"\"\"\n\u001b[1;32m    772\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[0;32m--> 538\u001b[0;31m                 self.target_id, self.name)\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    299\u001b[0m                     \u001b[0;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 raise Py4JError(\n",
        "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job 24 cancelled because Stage 43 was cancelled\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1280)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1226)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply$mcVI$sp(DAGScheduler.scala:1214)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply(DAGScheduler.scala:1213)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply(DAGScheduler.scala:1213)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofInt.foreach(ArrayOps.scala:156)\n\tat org.apache.spark.scheduler.DAGScheduler.handleStageCancellation(DAGScheduler.scala:1213)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1466)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1455)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1444)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1813)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1826)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1839)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1910)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:905)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:904)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:373)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:483)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x_test = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      "y_test = sc.parallelize([(\"a\", 2), (\"a\", 2), (\"c\", 8)])\n",
      "\n",
      "def reducer(tup):\n",
      "    left_val = tup[0]\n",
      "    right_vals = tup[1]\n",
      "    if len(left_val) == 0:\n",
      "        return list(right_vals)[0]\n",
      "    elif len(right_vals) == 0:\n",
      "        return list(left_val)[0]\n",
      "    else:\n",
      "        return list(left_val)[0]\n",
      "\n",
      "\n",
      "wut1 = sc.parallelize([(1, 2), (1, 1), (1, 0), (1, 8), (1, 4), (7, 4), \n",
      "                       (8, 0), (8, 6), (8, 8), (8, 1), (8, 4), (8, 2), \n",
      "                       (3, 0), (3, 2), (3, 6), (3, 5), (3, 1), (3, 7), \n",
      "                       (3, 8), (3, 3), (0, 2), (0, 3), (0, 4), (0, 5), \n",
      "                       (0, 6), (0, 1), (0, 0), (0, 8), (4, 0), (4, 8), \n",
      "                       (4, 1), (4, 2), (4, 5), (4, 6), (4, 3), (6, 8), \n",
      "                       (6, 6), (6, 0), (6, 3), (6, 4), (6, 1), (6, 7), \n",
      "                       (6, 5), (6, 2), (5, 7), (5, 1), (5, 6), (5, 3), \n",
      "                       (5, 4), (5, 2), (5, 5), (5, 8), (2, 1), (2, 0), (2, 6)])\n",
      "wut2 = sc.parallelize([(7, 0)])\n",
      "\n",
      "print wut1.join(wut2).collect()\n",
      "#print x_test.cogroup(y_test).mapValues(reducer).collect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(7, (4, 0))]\n"
       ]
      }
     ],
     "prompt_number": 123
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}