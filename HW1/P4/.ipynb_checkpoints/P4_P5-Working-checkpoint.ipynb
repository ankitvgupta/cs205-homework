{
 "metadata": {
  "name": "",
  "signature": "sha256:fa5fdd1dda115cba6d1f5f2012bd56235cc49cda9c64da75e2bd52d9bd5f1e07"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import findspark\n",
      "print findspark.find()\n",
      "findspark.init()\n",
      "\n",
      "import pyspark\n",
      "sc = pyspark.SparkContext(appName=\"Spark1\")\n",
      "\n",
      "import numpy as np \n",
      "import itertools"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/usr/local/opt/apache-spark/libexec\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def copartitioned(RDD1, RDD2):\n",
      "    \"check if two RDDs are copartitioned\"\n",
      "    return RDD1.partitioner == RDD2.partitioner"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#######\n",
      "# Implements Breadth First Search\n",
      "# Arguments:\n",
      "# \tadj (KV RDD): Edge list. For example: [(1,2), (1, 3), (2,3), (3, 2)] is the graph where there is a directed edge from 1 to 2 and 1 to 3, edges in both directions between 2 and 3.\n",
      "# \tstart (string): Where the breadth first search will start\n",
      "#   If a stopNode is provided, then it will stop iterating once that node is found.\n",
      "# Returns:\n",
      "# \t(RDD) Distances to each point, and (RDD) list of all points that were not reachable.\n",
      "def bfs(adj, start, sc, numPartitions, list_of_nodes, stopNode=None):\n",
      "    accum = sc.accumulator(0)\n",
      "    print 'Adjacency partitions:', adj.getNumPartitions()\n",
      "    adj = adj.map(lambda x: x).partitionBy(numPartitions).cache()\n",
      "    def reducer(tup):\n",
      "        left_val = tup[0]\n",
      "        right_vals = tup[1]\n",
      "        if len(right_vals) == 0:\n",
      "            return list(left_val)[0]\n",
      "        elif list(left_val)[0] == -1:\n",
      "            return list(right_vals)[0]\n",
      "        else:\n",
      "            return list(left_val)[0]\n",
      "    \n",
      "    def divider(tup):\n",
      "        dist = tup[0]\n",
      "        node_vals = tup[1]\n",
      "        return [(nv, dist + 1) for nv in node_vals]\n",
      "\n",
      "\n",
      "    distances = adj.mapValues(lambda _: -1)\n",
      "\n",
      "    traversed = sc.parallelize([(start, 0)]).cache().partitionBy(numPartitions)\n",
      "    distances = distances.fullOuterJoin(traversed).mapValues(lambda (x, y): x if y == None else y).partitionBy(numPartitions).cache()\n",
      "    #print distances.take(100)\n",
      "    assert(copartitioned(adj, distances))\n",
      "    print 'Start traverse partitions:', distances.getNumPartitions()\n",
      "    farthest = 0\n",
      "    accum.add(1)\n",
      "    while accum.value != 0:\n",
      "        accum.value = 0\n",
      "        print \"\\n\\nOn iteration \", farthest, ' for ', start, '\\n\\n'\n",
      "        if stopNode != None and distances.lookup(stopNode)[0] != -1:\n",
      "            break\n",
      "        farthest_nodes = distances.filter(lambda (node, dist): dist == farthest)\n",
      "\n",
      "        joined_farthest_neighbors = farthest_nodes.join(adj, numPartitions)\n",
      "        neighbor_distances = joined_farthest_neighbors.values().flatMap(divider).partitionBy(numPartitions).cache()\n",
      "        assert(copartitioned(neighbor_distances, distances))\n",
      "        distances = distances.cogroup(neighbor_distances).mapValues(reducer).cache()\n",
      "        farthest += 1\n",
      "        distances.filter(lambda (x, dist): dist == farthest).foreach(lambda x: accum.add(1))\n",
      "        #print traversed.take(100)\n",
      "\n",
      "    print 'End tranverse partitions:', distances.getNumPartitions()\n",
      "    return distances, distances.filter(lambda (node, dist): dist < 0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Your code here\n",
      "#import pyspark\n",
      "#sc = pyspark.SparkContext(appName=\"Spark1\")\n",
      "#from P4_bfs import *\n",
      "\n",
      "# make spark shut the hell up\n",
      "logger = sc._jvm.org.apache.log4j\n",
      "logger.LogManager.getLogger(\"org\"). setLevel( logger.Level.ERROR )\n",
      "logger.LogManager.getLogger(\"akka\").setLevel( logger.Level.ERROR )\n",
      "\n",
      "import numpy as np \n",
      "import itertools\n",
      "\n",
      "def remove_quotes(s):\n",
      "    return s.replace('\"','')\n",
      "\n",
      "def get_pairs(arr):\n",
      "    return list(itertools.permutations(arr, 2))\n",
      "\n",
      "num_Partitions = 15\n",
      "\n",
      "wlist = sc.textFile('source.csv', num_Partitions)\n",
      "#Extract the strings and remove all unnessary quotes\n",
      "better = wlist.map(lambda x: x.split('\",\"'), True).map(lambda x: map(remove_quotes, x), True)\n",
      "character_to_book = better.map(lambda x: (x[0], x[1]))\n",
      "book_to_character = character_to_book.map(lambda (x, y): (y, x))\n",
      "book_to_characters = book_to_character.groupByKey()\n",
      "\n",
      "adjacent_heros = book_to_characters.flatMap(lambda (book, characters): get_pairs(characters)).distinct().partitionBy(num_Partitions)\n",
      "\n",
      "characters = adjacent_heros.groupByKey().partitionBy(num_Partitions)\n",
      "print characters.take(20)\n",
      "\n",
      "res1, res2 = bfs(characters, 'CAPTAIN AMERICA', sc, num_Partitions, characters)\n",
      "print res1.values().countByValue(), '\\n'\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(u'SIR', <pyspark.resultiterable.ResultIterable object at 0x1124fabd0>), (u'HYZAKTL', <pyspark.resultiterable.ResultIterable object at 0x1124fad90>), (u'GAMBIT/REMY LEBEAU ', <pyspark.resultiterable.ResultIterable object at 0x1124fadd0>), (u'POITIER, CLAUDE', <pyspark.resultiterable.ResultIterable object at 0x1124fae10>), (u'NINGAL', <pyspark.resultiterable.ResultIterable object at 0x1124fae50>), (u'HIGGINS, RICHIE', <pyspark.resultiterable.ResultIterable object at 0x1124fae90>), (u'GARTHAN SAAL', <pyspark.resultiterable.ResultIterable object at 0x1124faed0>), (u'FLATMAN', <pyspark.resultiterable.ResultIterable object at 0x1124faf10>), (u'PROFESSOR X | MUTANT', <pyspark.resultiterable.ResultIterable object at 0x1124faf50>), (u'BENWAY, DR.', <pyspark.resultiterable.ResultIterable object at 0x1124faf90>), (u'KANDRA', <pyspark.resultiterable.ResultIterable object at 0x1124fafd0>), (u'HEADLOK', <pyspark.resultiterable.ResultIterable object at 0x112567050>), (u'JONES, SANDY', <pyspark.resultiterable.ResultIterable object at 0x112567090>), (u'RITTER, DEBORAH ', <pyspark.resultiterable.ResultIterable object at 0x1125670d0>), (u'MANDARIN', <pyspark.resultiterable.ResultIterable object at 0x112567110>), (u'BULLET BIKER', <pyspark.resultiterable.ResultIterable object at 0x112567150>), (u'DR. FAUSTUS', <pyspark.resultiterable.ResultIterable object at 0x112567190>), (u'GREEN GOBLIN V/', <pyspark.resultiterable.ResultIterable object at 0x1125671d0>), (u'ENRAGED', <pyspark.resultiterable.ResultIterable object at 0x112567210>), (u'RED WOLF III/WILL TA', <pyspark.resultiterable.ResultIterable object at 0x112567250>)]\n",
        "Adjacency partitions: 15\n",
        "Start traverse partitions:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 15\n",
        "\n",
        "\n",
        "On iteration  0  for  CAPTAIN AMERICA \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "On iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1  for  CAPTAIN AMERICA \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "On iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2  for  CAPTAIN AMERICA \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "On iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3  for  CAPTAIN AMERICA \n",
        "\n",
        "\n",
        "End tranverse partitions:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 15\n",
        "defaultdict(<type 'int'>, {0: 1, 1: 1906, 2: 4463, 3: 38, -1: 18})"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \n",
        "\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#linklist = sc.textFile('../P5/generated_links_representative_sorted.txt', 32)\n",
      "#titlelist = sc.textFile('../P5/generated_titles_representative_sorted.txt', 32)\n",
      "linklist = sc.textFile('../P5/generated_links_small_sorted.txt', 32)\n",
      "titlelist = sc.textFile('../P5/generated_titles_small_sorted.txt', 32)\n",
      "numerical_titles = titlelist.zipWithIndex().cache()\n",
      "\n",
      "num_nodes = numerical_titles.count()\n",
      "#num_Partitions = int(num_nodes/50)\n",
      "num_Partitions = 128\n",
      "\n",
      "# Borrowed from Professor's Github example on SparkPageRank\n",
      "def link_string_to_KV(s):\n",
      "    src, dests = s.split(': ')\n",
      "    dests = [int(to) - 1 for to in dests.split(' ')]\n",
      "    return (int(src) - 1, dests)\n",
      "\n",
      "split_list = linklist.map(link_string_to_KV).cache()\n",
      "nodes = split_list.map(lambda (x,y): int(x), True)\n",
      "assert(copartitioned(split_list, nodes))\n",
      "\n",
      "#start = \"Kevin_Bacon\"\n",
      "#end = \"Harvard_University\"\n",
      "start = 'TITLE_8'\n",
      "end = \"TITLE_4\"\n",
      "\n",
      "start_node = numerical_titles.lookup(start)[0]\n",
      "end_node = numerical_titles.lookup(end)[0]\n",
      "print start_node, end_node\n",
      "\n",
      "distances, unreachables = bfs(split_list, start_node, sc, num_Partitions, nodes, end_node)\n",
      "#print distances.values().countByValue(), '\\n'\n",
      "print distances.take(100)\n",
      "print \"Distance to end node:\", distances.map(lambda x: x).lookup(end_node)[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "7 3\n",
        "Adjacency partitions: 42\n",
        "Start traverse partitions:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 128\n",
        "\n",
        "\n",
        "On iteration  0  for  7 \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "On iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1  for  7 \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "On iteration "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2  for  7 \n",
        "\n",
        "\n",
        "End tranverse partitions: 128\n",
        "[(0, 1), (1, 1), (2, 1), (3, 2), (4, 1), (5, 1), (6, 1), (7, 0), (8, 1)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Distance to end node: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x_test = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      "y_test = sc.parallelize([(\"a\", 2), (\"a\", 2), (\"c\", 8)])\n",
      "\n",
      "def reducer(tup):\n",
      "    left_val = tup[0]\n",
      "    right_vals = tup[1]\n",
      "    if len(left_val) == 0:\n",
      "        return list(right_vals)[0]\n",
      "    elif len(right_vals) == 0:\n",
      "        return list(left_val)[0]\n",
      "    else:\n",
      "        return list(left_val)[0]\n",
      "\n",
      "\n",
      "wut1 = sc.parallelize([(1, 2), (1, 1), (1, 0), (1, 8), (1, 4), (7, 4), \n",
      "                       (8, 0), (8, 6), (8, 8), (8, 1), (8, 4), (8, 2), \n",
      "                       (3, 0), (3, 2), (3, 6), (3, 5), (3, 1), (3, 7), \n",
      "                       (3, 8), (3, 3), (0, 2), (0, 3), (0, 4), (0, 5), \n",
      "                       (0, 6), (0, 1), (0, 0), (0, 8), (4, 0), (4, 8), \n",
      "                       (4, 1), (4, 2), (4, 5), (4, 6), (4, 3), (6, 8), \n",
      "                       (6, 6), (6, 0), (6, 3), (6, 4), (6, 1), (6, 7), \n",
      "                       (6, 5), (6, 2), (5, 7), (5, 1), (5, 6), (5, 3), \n",
      "                       (5, 4), (5, 2), (5, 5), (5, 8), (2, 1), (2, 0), (2, 6)])\n",
      "wut2 = sc.parallelize([(7, 0)])\n",
      "\n",
      "print wut1.join(wut2).collect()\n",
      "#print x_test.cogroup(y_test).mapValues(reducer).collect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(7, (4, 0))]\n"
       ]
      }
     ],
     "prompt_number": 123
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}